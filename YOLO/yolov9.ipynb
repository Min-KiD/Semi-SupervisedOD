{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2f0eff",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-26T10:03:11.684544Z",
     "iopub.status.busy": "2024-05-26T10:03:11.684264Z",
     "iopub.status.idle": "2024-05-26T10:03:16.818667Z",
     "shell.execute_reply": "2024-05-26T10:03:16.817634Z"
    },
    "papermill": {
     "duration": 5.142326,
     "end_time": "2024-05-26T10:03:16.821075",
     "exception": false,
     "start_time": "2024-05-26T10:03:11.678749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import yaml\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eb20302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T10:03:16.831095Z",
     "iopub.status.busy": "2024-05-26T10:03:16.830715Z",
     "iopub.status.idle": "2024-05-26T10:03:16.876652Z",
     "shell.execute_reply": "2024-05-26T10:03:16.875636Z"
    },
    "papermill": {
     "duration": 0.053161,
     "end_time": "2024-05-26T10:03:16.878741",
     "exception": false,
     "start_time": "2024-05-26T10:03:16.825580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of instances: 1232\n"
     ]
    }
   ],
   "source": [
    "json_file_path = '/kaggle/input/semis-od-coco-10/instances_val2017.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Extract the annotations\n",
    "annotations = data.get('images', [])\n",
    "\n",
    "# Count the number of instances\n",
    "num_instances = len(annotations)\n",
    "\n",
    "print(f'Total number of instances: {num_instances}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b8618c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T10:03:16.888319Z",
     "iopub.status.busy": "2024-05-26T10:03:16.887804Z",
     "iopub.status.idle": "2024-05-26T10:03:16.893222Z",
     "shell.execute_reply": "2024-05-26T10:03:16.892357Z"
    },
    "papermill": {
     "duration": 0.012423,
     "end_time": "2024-05-26T10:03:16.895356",
     "exception": false,
     "start_time": "2024-05-26T10:03:16.882933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"images\": [{\"license\": 4, \"file_name\": \"000000149356.jpg\", \"coco_url\": \"http://images.cocodataset.org/train2017/000000149356.jpg\", \"height\": 428, \"width\": 640, \"date_captured\": \"2013-11-18 13:21:30\", \"flickr_url\": \"http://farm4.staticflickr.com/3349/3579581024_25d8e24a6c_z.jpg\", \"id\": 149356}, {\"license\": 3, \"file_name\": \"000000498400.jpg\", \"coco_url\": \"http://images.cocodataset.org/train2017/000000498400.jpg\", \"height\": 427, \"width\": 640, \"date_captured\": \"2013-11-17 17:10:19\", \"flickr_url\": \"http://farm8.staticflickr.com/7184/6892294630_44d92b1621_z.jpg\", \"id\": 498400}, {\"license\": 2, \"file_name\": \"000000498943.jpg\", \"coco_url\": \"http://images.cocodataset.org/train2017/000000498943.jpg\", \"height\": 444, \"width\": 640, \"date_captured\": \"2013-11-17 02:17:22\", \"flickr_url\": \"http://farm5.staticflickr.com/4049/4532685478_139af40701_z.jpg\", \"id\": 498943}, {\"license\": 4, \"file_name\": \"000000452205.jpg\", \"coco_url\": \"http://images.cocodataset.org/train2017/000000452205.jpg\", \"height\": 428, \"\n"
     ]
    }
   ],
   "source": [
    "file_path = '/kaggle/input/semis-od-coco-10/instances_val2017.json'\n",
    "\n",
    "# Open and read the first 100 characters of the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    first_100_chars = file.read(1000)\n",
    "\n",
    "# Print the first 100 characters\n",
    "print(first_100_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6846fa2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T10:03:16.905667Z",
     "iopub.status.busy": "2024-05-26T10:03:16.905388Z",
     "iopub.status.idle": "2024-05-26T10:03:55.902461Z",
     "shell.execute_reply": "2024-05-26T10:03:55.901593Z"
    },
    "papermill": {
     "duration": 39.004884,
     "end_time": "2024-05-26T10:03:55.904958",
     "exception": false,
     "start_time": "2024-05-26T10:03:16.900074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths\n",
    "coco_path = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n",
    "train_json = '/kaggle/input/semis-od-coco-10/instances_train2017_labeled.json'\n",
    "val_json = '/kaggle/input/semis-od-coco-10/instances_val2017.json'\n",
    "output_path = '/kaggle/working/yolo_dataset'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(output_path, 'images', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'images', 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'labels', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'labels', 'val'), exist_ok=True)\n",
    "\n",
    "# Load annotations\n",
    "with open(train_json) as f:\n",
    "    train_annotations = json.load(f)\n",
    "\n",
    "with open(val_json) as f:\n",
    "    val_annotations = json.load(f)\n",
    "\n",
    "# Extract categories\n",
    "categories = {category['id']: category['name'] for category in train_annotations['categories']}\n",
    "\n",
    "# Function to convert COCO bbox to YOLO format\n",
    "def convert_bbox(size, box):\n",
    "    dw = 1. / size[0]\n",
    "    dh = 1. / size[1]\n",
    "    x = (box[0] + box[2] / 2.0) * dw\n",
    "    y = (box[1] + box[3] / 2.0) * dh\n",
    "    w = box[2] * dw\n",
    "    h = box[3] * dh\n",
    "    return (x, y, w, h)\n",
    "\n",
    "# Function to process annotations\n",
    "def process_annotations(annotations, split):\n",
    "    images = {image['id']: image for image in annotations['images']}\n",
    "    seen_labels = defaultdict(set)  # Track seen labels for each image\n",
    "    for ann in annotations['annotations']:\n",
    "        image = images[ann['image_id']]\n",
    "        bbox = convert_bbox((image['width'], image['height']), ann['bbox'])\n",
    "        label = ann['category_id'] - 2   # YOLO class ids start at 0\n",
    "        if (label < 0 or label > 7):\n",
    "            print(label)\n",
    "        bbox_tuple = (label, *bbox)  # Create a tuple of label and bbox for comparison\n",
    "\n",
    "        if bbox_tuple not in seen_labels[image['id']]:  # Check for duplicates\n",
    "            seen_labels[image['id']].add(bbox_tuple)\n",
    "            # Write label file\n",
    "            label_path = os.path.join(output_path, 'labels', split, f\"{os.path.splitext(image['file_name'])[0]}.txt\")\n",
    "            with open(label_path, 'a') as f:\n",
    "                f.write(f\"{label} \" + \" \".join(map(str, bbox)) + '\\n')\n",
    "\n",
    "        # Copy image to split folder if not already copied\n",
    "        target_image_path = os.path.join(output_path, 'images', split, image['file_name'])\n",
    "        if not os.path.exists(target_image_path):\n",
    "            shutil.copy(os.path.join(coco_path, image['file_name']), target_image_path)\n",
    "\n",
    "# Process train and val annotations\n",
    "process_annotations(train_annotations, 'train')\n",
    "process_annotations(val_annotations, 'val')\n",
    "\n",
    "# Create .yaml file\n",
    "yaml_content = f\"\"\"\n",
    "train: {os.path.join(output_path, 'images', 'train')}\n",
    "val: {os.path.join(output_path, 'images', 'val')}\n",
    "\n",
    "nc: {len(categories)}\n",
    "names: {list(categories.values())}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(output_path, 'dataset.yaml'), 'w') as f:\n",
    "    f.write(yaml_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defad2ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T10:03:55.915998Z",
     "iopub.status.busy": "2024-05-26T10:03:55.915649Z",
     "iopub.status.idle": "2024-05-26T10:03:55.923753Z",
     "shell.execute_reply": "2024-05-26T10:03:55.922626Z"
    },
    "papermill": {
     "duration": 0.015944,
     "end_time": "2024-05-26T10:03:55.925810",
     "exception": false,
     "start_time": "2024-05-26T10:03:55.909866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': '/kaggle/working/yolo_dataset/images/train', 'val': '/kaggle/working/yolo_dataset/images/val', 'nc': 8, 'names': ['bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat']}\n"
     ]
    }
   ],
   "source": [
    "def read_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "file_path = '/kaggle/working/yolo_dataset/dataset.yaml'  # Replace with your file path\n",
    "yaml_data = read_yaml(file_path)\n",
    "print(yaml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb79d493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T10:03:55.936291Z",
     "iopub.status.busy": "2024-05-26T10:03:55.936036Z",
     "iopub.status.idle": "2024-05-26T10:03:59.446374Z",
     "shell.execute_reply": "2024-05-26T10:03:59.445348Z"
    },
    "papermill": {
     "duration": 3.518011,
     "end_time": "2024-05-26T10:03:59.448667",
     "exception": false,
     "start_time": "2024-05-26T10:03:55.930656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov9'...\r\n",
      "remote: Enumerating objects: 668, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (294/294), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (93/93), done.\u001b[K\r\n",
      "remote: Total 668 (delta 224), reused 201 (delta 201), pack-reused 374\u001b[K\r\n",
      "Receiving objects: 100% (668/668), 3.22 MiB | 16.84 MiB/s, done.\r\n",
      "Resolving deltas: 100% (269/269), done.\r\n",
      "/kaggle/working/yolov9\n",
      "--2024-05-26 10:03:58--  https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt\r\n",
      "Resolving github.com (github.com)... 140.82.113.4\r\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/759338070/c8ca43f2-0d2d-4aa3-a074-426505bfbfb1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240526%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240526T100358Z&X-Amz-Expires=300&X-Amz-Signature=58e80c8d3cd4cb92ee5a2666de49179b915038a783a51e8c2265d751f41dac81&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=759338070&response-content-disposition=attachment%3B%20filename%3Dyolov9-c.pt&response-content-type=application%2Foctet-stream [following]\r\n",
      "--2024-05-26 10:03:58--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/759338070/c8ca43f2-0d2d-4aa3-a074-426505bfbfb1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240526%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240526T100358Z&X-Amz-Expires=300&X-Amz-Signature=58e80c8d3cd4cb92ee5a2666de49179b915038a783a51e8c2265d751f41dac81&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=759338070&response-content-disposition=attachment%3B%20filename%3Dyolov9-c.pt&response-content-type=application%2Foctet-stream\r\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\r\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 103153312 (98M) [application/octet-stream]\r\n",
      "Saving to: 'yolov9-c.pt'\r\n",
      "\r\n",
      "yolov9-c.pt         100%[===================>]  98.37M   236MB/s    in 0.4s    \r\n",
      "\r\n",
      "2024-05-26 10:03:59 (236 MB/s) - 'yolov9-c.pt' saved [103153312/103153312]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov9.git\n",
    "%cd yolov9\n",
    "!wget https://github.com/WongKinYiu/yolov9/releases/download/v0.1/yolov9-c.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655bd98b",
   "metadata": {
    "papermill": {
     "duration": 0.005985,
     "end_time": "2024-05-26T10:03:59.461329",
     "exception": false,
     "start_time": "2024-05-26T10:03:59.455344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Trainning with multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2defeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T10:03:59.475439Z",
     "iopub.status.busy": "2024-05-26T10:03:59.474633Z",
     "iopub.status.idle": "2024-05-26T11:00:56.064172Z",
     "shell.execute_reply": "2024-05-26T11:00:56.063050Z"
    },
    "papermill": {
     "duration": 3416.599248,
     "end_time": "2024-05-26T11:00:56.066536",
     "exception": false,
     "start_time": "2024-05-26T10:03:59.467288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-26 10:04:02,333] torch.distributed.run: [WARNING] \r\n",
      "[2024-05-26 10:04:02,333] torch.distributed.run: [WARNING] *****************************************\r\n",
      "[2024-05-26 10:04:02,333] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "[2024-05-26 10:04:02,333] torch.distributed.run: [WARNING] *****************************************\r\n",
      "2024-05-26 10:04:13.287071: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-26 10:04:13.287062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-05-26 10:04:13.287700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-26 10:04:13.287716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-05-26 10:04:13.470517: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2024-05-26 10:04:13.470528: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\r\n",
      "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=./yolov9-c.pt, cfg=models/detect/yolov9-c.yaml, data=/kaggle/working/yolo_dataset/dataset.yaml, hyp=hyp.scratch-high.yaml, epochs=15, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0,1, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=True, workers=8, project=runs/train, name=yolov9-c, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=15, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\n",
      "YOLO ðŸš€ v0.1-89-g93f1a28 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\r\n",
      "                                                    CUDA:1 (Tesla T4, 15102MiB)\r\n",
      "\r\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\r\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO ðŸš€ in ClearML\r\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO ðŸš€ runs in Comet\r\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\r\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 14.8MB/s]\r\n",
      "Overriding model.yaml nc=80 with nc=8\r\n",
      "\r\n",
      "                 from  n    params  module                                  arguments                     \r\n",
      "  0                -1  1         0  models.common.Silence                   []                            \r\n",
      "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \r\n",
      "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n",
      "  3                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \r\n",
      "  4                -1  1    164352  models.common.ADown                     [256, 256]                    \r\n",
      "  5                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \r\n",
      "  6                -1  1    656384  models.common.ADown                     [512, 512]                    \r\n",
      "  7                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \r\n",
      "  8                -1  1    656384  models.common.ADown                     [512, 512]                    \r\n",
      "  9                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \r\n",
      " 10                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \r\n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 12           [-1, 7]  1         0  models.common.Concat                    [1]                           \r\n",
      " 13                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \r\n",
      " 14                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 15           [-1, 5]  1         0  models.common.Concat                    [1]                           \r\n",
      " 16                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \r\n",
      " 17                -1  1    164352  models.common.ADown                     [256, 256]                    \r\n",
      " 18          [-1, 13]  1         0  models.common.Concat                    [1]                           \r\n",
      " 19                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \r\n",
      " 20                -1  1    656384  models.common.ADown                     [512, 512]                    \r\n",
      " 21          [-1, 10]  1         0  models.common.Concat                    [1]                           \r\n",
      " 22                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \r\n",
      " 23                 5  1    131328  models.common.CBLinear                  [512, [256]]                  \r\n",
      " 24                 7  1    393984  models.common.CBLinear                  [512, [256, 512]]             \r\n",
      " 25                 9  1    656640  models.common.CBLinear                  [512, [256, 512, 512]]        \r\n",
      " 26                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \r\n",
      " 27                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n",
      " 28                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \r\n",
      " 29                -1  1    164352  models.common.ADown                     [256, 256]                    \r\n",
      " 30  [23, 24, 25, -1]  1         0  models.common.CBFuse                    [[0, 0, 0]]                   \r\n",
      " 31                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \r\n",
      " 32                -1  1    656384  models.common.ADown                     [512, 512]                    \r\n",
      " 33      [24, 25, -1]  1         0  models.common.CBFuse                    [[1, 1]]                      \r\n",
      " 34                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \r\n",
      " 35                -1  1    656384  models.common.ADown                     [512, 512]                    \r\n",
      " 36          [25, -1]  1         0  models.common.CBFuse                    [[2]]                         \r\n",
      " 37                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \r\n",
      " 38[31, 34, 37, 16, 19, 22]  1  21558992  models.yolo.DualDDetect                 [8, [512, 512, 512, 256, 512, 512]]\r\n",
      "yolov9-c summary: 962 layers, 51015760 parameters, 51015728 gradients\r\n",
      "\r\n",
      "Transferred 1448/1460 items from yolov9-c.pt\r\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\r\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 238 weight(decay=0.0), 255 weight(decay=0.0005), 253 bias\r\n",
      "Using SyncBatchNorm()\r\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/yolo_dataset/labels/train... 2735 images, 0 back\u001b[0m\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/yolo_dataset/labels/train.cache\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/yolo_dataset/labels/val... 1232 images, 0 backgrou\u001b[0m\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/yolo_dataset/labels/val.cache\r\n",
      "Plotting labels to runs/train/yolov9-c2/labels.jpg... \r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\r\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\r\n",
      "Image sizes 640 train, 640 val\r\n",
      "Using 4 dataloader workers\r\n",
      "Logging results to \u001b[1mruns/train/yolov9-c2\u001b[0m\r\n",
      "Starting training for 15 epochs...\r\n",
      "Closing dataloader mosaic\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       0/14        10G      1.276      2.466      1.445         20        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.753      0.626      0.698      0.508\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       1/14      12.2G      1.415      1.943      1.558         27        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.613      0.488      0.527      0.356\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       2/14      12.2G      1.676      2.321      1.762         38        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.381      0.344      0.294      0.172\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       3/14      12.2G      1.964      2.746      1.943         40        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.485      0.289      0.285      0.158\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       4/14      12.2G      2.041      2.883      2.031         14        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.377      0.302      0.259      0.144\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       5/14      12.2G      2.023      2.736      2.013         14        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.432      0.304      0.306      0.177\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       6/14      12.2G       1.99      2.622      1.983         30        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.559      0.375      0.398      0.242\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       7/14      12.2G      1.899      2.463      1.952         18        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.532      0.387      0.405      0.244\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       8/14      12.2G      1.846      2.336      1.886         42        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.526      0.382      0.403      0.245\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "       9/14      12.2G      1.771      2.161      1.845         26        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.554      0.392      0.427      0.269\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "      10/14      12.2G      1.718      2.089      1.808         23        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.626      0.439      0.495      0.323\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "      11/14      12.2G      1.694      1.987      1.768         29        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301       0.63      0.479      0.524      0.345\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "      12/14      12.2G        1.6        1.8      1.714         40        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.627      0.483      0.536      0.358\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "      13/14      12.2G       1.53      1.675      1.645         46        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.669      0.529      0.582       0.39\r\n",
      "\r\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n",
      "      14/14      12.2G      1.478      1.549       1.62         40        640: 1\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.711      0.525      0.596      0.404\r\n",
      "\r\n",
      "15 epochs completed in 0.913 hours.\r\n",
      "Optimizer stripped from runs/train/yolov9-c2/weights/last.pt, 102.8MB\r\n",
      "Optimizer stripped from runs/train/yolov9-c2/weights/best.pt, 102.8MB\r\n",
      "\r\n",
      "Validating runs/train/yolov9-c2/weights/best.pt...\r\n",
      "Fusing layers... \r\n",
      "yolov9-c summary: 604 layers, 50714448 parameters, 0 gradients\r\n",
      "                 Class     Images  Instances          P          R      mAP50   \r\n",
      "                   all       1232       4301      0.752      0.626      0.697      0.508\r\n",
      "               bicycle       1232        364      0.749      0.497      0.595      0.376\r\n",
      "                   car       1232       2054      0.715      0.626      0.671      0.449\r\n",
      "            motorcycle       1232        356      0.769      0.596      0.671       0.46\r\n",
      "              airplane       1232        180      0.775      0.817      0.846      0.616\r\n",
      "                   bus       1232        283      0.792      0.781      0.825      0.699\r\n",
      "                 train       1232        227      0.923      0.793       0.89      0.716\r\n",
      "                 truck       1232        420      0.583        0.5      0.532      0.419\r\n",
      "                  boat       1232        417      0.712        0.4      0.545      0.325\r\n",
      "Results saved to \u001b[1mruns/train/yolov9-c2\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun \\\n",
    "    --nproc_per_node 2 \\\n",
    "    --master_port 9527 \\\n",
    "    train_dual.py \\\n",
    "    --workers 8 \\\n",
    "    --device 0,1 \\\n",
    "    --sync-bn \\\n",
    "    --batch 16 \\\n",
    "    --data /kaggle/working/yolo_dataset/dataset.yaml \\\n",
    "    --img 640 \\\n",
    "    --cfg models/detect/yolov9-c.yaml \\\n",
    "    --weights './yolov9-c.pt' \\\n",
    "    --name yolov9-c \\\n",
    "    --hyp hyp.scratch-high.yaml \\\n",
    "    --min-items 0 \\\n",
    "    --epochs 15 \\\n",
    "    --close-mosaic 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd546a",
   "metadata": {
    "papermill": {
     "duration": 0.330668,
     "end_time": "2024-05-26T11:00:56.730859",
     "exception": false,
     "start_time": "2024-05-26T11:00:56.400191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Trainning with 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a36061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T11:00:57.390889Z",
     "iopub.status.busy": "2024-05-26T11:00:57.390475Z",
     "iopub.status.idle": "2024-05-26T11:00:57.395277Z",
     "shell.execute_reply": "2024-05-26T11:00:57.394394Z"
    },
    "papermill": {
     "duration": 0.33541,
     "end_time": "2024-05-26T11:00:57.397167",
     "exception": false,
     "start_time": "2024-05-26T11:00:57.061757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python train_dual.py \\\n",
    "#     --workers 8 \\\n",
    "#     --device 0 \\\n",
    "#     --batch 8 \\\n",
    "#     --data /kaggle/working/yolo_dataset/dataset.yaml \\\n",
    "#     --img 640 \\\n",
    "#     --cfg models/detect/yolov9-c.yaml \\\n",
    "#     --weights './yolov9-c.pt' \\\n",
    "#     --name yolov9-c \\\n",
    "#     --hyp hyp.scratch-high.yaml \\\n",
    "#     --min-items 0 \\\n",
    "#     --epochs 5 \\\n",
    "#     --close-mosaic 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df3bbbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T11:00:58.107388Z",
     "iopub.status.busy": "2024-05-26T11:00:58.107053Z",
     "iopub.status.idle": "2024-05-26T11:01:05.853158Z",
     "shell.execute_reply": "2024-05-26T11:01:05.852134Z"
    },
    "papermill": {
     "duration": 8.131354,
     "end_time": "2024-05-26T11:01:05.855650",
     "exception": false,
     "start_time": "2024-05-26T11:00:57.724296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval_dual: \u001b[0mdata=/kaggle/working/yolo_dataset/dataset.yaml, weights=['runs/train/yolov9-c3/weights/best.pt'], batch_size=16, imgsz=640, conf_thres=0.001, iou_thres=0.7, max_det=300, task=val, device=0, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False, min_items=0\r\n",
      "YOLO ðŸš€ v0.1-89-g93f1a28 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/yolov9/val_dual.py\", line 393, in <module>\r\n",
      "    main(opt)\r\n",
      "  File \"/kaggle/working/yolov9/val_dual.py\", line 366, in main\r\n",
      "    run(**vars(opt))\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/kaggle/working/yolov9/val_dual.py\", line 122, in run\r\n",
      "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\r\n",
      "  File \"/kaggle/working/yolov9/models/common.py\", line 684, in __init__\r\n",
      "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\r\n",
      "  File \"/kaggle/working/yolov9/models/experimental.py\", line 243, in attempt_load\r\n",
      "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 986, in load\r\n",
      "    with _open_file_like(f, 'rb') as opened_file:\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 435, in _open_file_like\r\n",
      "    return _open_file(name_or_buffer, mode)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 416, in __init__\r\n",
      "    super().__init__(open(name, mode))\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'runs/train/yolov9-c3/weights/best.pt'\r\n"
     ]
    }
   ],
   "source": [
    "!python val_dual.py \\\n",
    "    --img 640 \\\n",
    "    --batch 16 \\\n",
    "    --conf 0.001 \\\n",
    "    --iou 0.7 \\\n",
    "    --device 0 \\\n",
    "    --data /kaggle/working/yolo_dataset/dataset.yaml \\\n",
    "    --weights runs/train/yolov9-c3/weights/best.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d0de4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T11:01:06.520992Z",
     "iopub.status.busy": "2024-05-26T11:01:06.520141Z",
     "iopub.status.idle": "2024-05-26T11:01:14.013518Z",
     "shell.execute_reply": "2024-05-26T11:01:14.012261Z"
    },
    "papermill": {
     "duration": 7.828119,
     "end_time": "2024-05-26T11:01:14.015926",
     "exception": false,
     "start_time": "2024-05-26T11:01:06.187807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect_dual: \u001b[0mweights=['runs/train/yolov9-c3/weights/best.pt'], source=/kaggle/input/coco-2017-dataset/coco2017/train2017/000000461973.jpg, data=/kaggle/working/yolo_dataset/dataset.yaml, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\r\n",
      "YOLO ðŸš€ v0.1-89-g93f1a28 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/yolov9/detect_dual.py\", line 232, in <module>\r\n",
      "    main(opt)\r\n",
      "  File \"/kaggle/working/yolov9/detect_dual.py\", line 227, in main\r\n",
      "    run(**vars(opt))\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n",
      "    return func(*args, **kwargs)\r\n",
      "  File \"/kaggle/working/yolov9/detect_dual.py\", line 68, in run\r\n",
      "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\r\n",
      "  File \"/kaggle/working/yolov9/models/common.py\", line 684, in __init__\r\n",
      "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\r\n",
      "  File \"/kaggle/working/yolov9/models/experimental.py\", line 243, in attempt_load\r\n",
      "    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 986, in load\r\n",
      "    with _open_file_like(f, 'rb') as opened_file:\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 435, in _open_file_like\r\n",
      "    return _open_file(name_or_buffer, mode)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/serialization.py\", line 416, in __init__\r\n",
      "    super().__init__(open(name, mode))\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'runs/train/yolov9-c3/weights/best.pt'\r\n"
     ]
    }
   ],
   "source": [
    "!python3 detect_dual.py \\\n",
    "    --img 640 \\\n",
    "    --conf 0.6 \\\n",
    "    --weights runs/train/yolov9-c3/weights/best.pt \\\n",
    "    --data /kaggle/working/yolo_dataset/dataset.yaml \\\n",
    "    --source '/kaggle/input/coco-2017-dataset/coco2017/train2017/000000461973.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb9b85",
   "metadata": {
    "papermill": {
     "duration": 0.330145,
     "end_time": "2024-05-26T11:01:14.682611",
     "exception": false,
     "start_time": "2024-05-26T11:01:14.352466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4857582,
     "sourceId": 8206445,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3487.173393,
   "end_time": "2024-05-26T11:01:15.847109",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-26T10:03:08.673716",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
