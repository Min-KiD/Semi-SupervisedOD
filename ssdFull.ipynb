{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c728e8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-08T22:49:38.980666Z",
     "iopub.status.busy": "2024-06-08T22:49:38.979864Z",
     "iopub.status.idle": "2024-06-08T22:49:39.708492Z",
     "shell.execute_reply": "2024-06-08T22:49:39.707674Z"
    },
    "papermill": {
     "duration": 0.737011,
     "end_time": "2024-06-08T22:49:39.710772",
     "exception": false,
     "start_time": "2024-06-08T22:49:38.973761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c4fc4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T22:49:39.720226Z",
     "iopub.status.busy": "2024-06-08T22:49:39.719860Z",
     "iopub.status.idle": "2024-06-08T22:49:53.281944Z",
     "shell.execute_reply": "2024-06-08T22:49:53.280942Z"
    },
    "papermill": {
     "duration": 13.569274,
     "end_time": "2024-06-08T22:49:53.284382",
     "exception": false,
     "start_time": "2024-06-08T22:49:39.715108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\r\n",
      "  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\r\n",
      "Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pycocotools\r\n",
      "Successfully installed pycocotools-2.0.7\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5313a55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T22:49:53.295310Z",
     "iopub.status.busy": "2024-06-08T22:49:53.294991Z",
     "iopub.status.idle": "2024-06-08T22:49:55.913391Z",
     "shell.execute_reply": "2024-06-08T22:49:55.912600Z"
    },
    "papermill": {
     "duration": 2.626682,
     "end_time": "2024-06-08T22:49:55.915819",
     "exception": false,
     "start_time": "2024-06-08T22:49:53.289137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def combine_coco_jsons(json_path1, json_path2, output_path):\n",
    "    # Load the first JSON file\n",
    "    with open(json_path1, 'r') as f:\n",
    "        data1 = json.load(f)\n",
    "\n",
    "    # Load the second JSON file\n",
    "    with open(json_path2, 'r') as f:\n",
    "        data2 = json.load(f)\n",
    "    \n",
    "    # Initialize the combined data structure\n",
    "    combined_data = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": data1[\"categories\"]\n",
    "    }\n",
    "\n",
    "    # Combine images\n",
    "    image_id_map = {}\n",
    "    new_image_id = 1\n",
    "    for image in data1[\"images\"]:\n",
    "        image_id_map[image[\"id\"]] = new_image_id\n",
    "        image[\"id\"] = new_image_id\n",
    "        combined_data[\"images\"].append(image)\n",
    "        new_image_id += 1\n",
    "    \n",
    "    for image in data2[\"images\"]:\n",
    "        image_id_map[image[\"id\"]] = new_image_id\n",
    "        image[\"id\"] = new_image_id\n",
    "        combined_data[\"images\"].append(image)\n",
    "        new_image_id += 1\n",
    "\n",
    "    # Combine annotations\n",
    "    new_annotation_id = 1\n",
    "    for annotation in data1[\"annotations\"]:\n",
    "        annotation[\"id\"] = new_annotation_id\n",
    "        annotation[\"image_id\"] = image_id_map[annotation[\"image_id\"]]\n",
    "        combined_data[\"annotations\"].append(annotation)\n",
    "        new_annotation_id += 1\n",
    "    \n",
    "    for annotation in data2[\"annotations\"]:\n",
    "        annotation[\"id\"] = new_annotation_id\n",
    "        annotation[\"image_id\"] = image_id_map[annotation[\"image_id\"]]\n",
    "        combined_data[\"annotations\"].append(annotation)\n",
    "        new_annotation_id += 1\n",
    "\n",
    "    # Save the combined JSON\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(combined_data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "json_path1 = '/kaggle/input/semis-od-coco-10/instances_train2017_labeled.json'\n",
    "json_path2 = '/kaggle/input/semis-od-coco-10/yolov9semi/instances_train2017_unlabeled_predicted.json'\n",
    "output_path = '/kaggle/working/train_semi2017.json'\n",
    "\n",
    "combine_coco_jsons(json_path1, json_path2, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3163ed22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T22:49:55.926915Z",
     "iopub.status.busy": "2024-06-08T22:49:55.926613Z",
     "iopub.status.idle": "2024-06-08T22:50:02.145744Z",
     "shell.execute_reply": "2024-06-08T22:50:02.144956Z"
    },
    "papermill": {
     "duration": 6.226988,
     "end_time": "2024-06-08T22:50:02.147955",
     "exception": false,
     "start_time": "2024-06-08T22:49:55.920967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        coco_annotations = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, self.coco.imgs[img_id]['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for annotation in coco_annotations:\n",
    "            x, y, w, h = annotation['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(annotation['category_id'])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target, img_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        image = transforms.ToTensor()(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, flip_prob):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.flip_prob:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = image.size(2) - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b91e43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T22:50:02.158973Z",
     "iopub.status.busy": "2024-06-08T22:50:02.158574Z",
     "iopub.status.idle": "2024-06-08T22:50:02.750631Z",
     "shell.execute_reply": "2024-06-08T22:50:02.749560Z"
    },
    "papermill": {
     "duration": 0.600024,
     "end_time": "2024-06-08T22:50:02.752943",
     "exception": false,
     "start_time": "2024-06-08T22:50:02.152919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "    \n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(ToTensor())\n",
    "    if train:\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return Compose(transforms)\n",
    "\n",
    "train_dataset = COCODataset('/kaggle/input/coco-2017-dataset/coco2017/train2017',\n",
    "                            '/kaggle/working/train_semi2017.json',\n",
    "                            transform=get_transform(train=True))\n",
    "\n",
    "# train_dataset = COCODataset('/kaggle/input/coco-2017-dataset/coco2017/train2017',\n",
    "#                             '/kaggle/input/semis-od-coco-10/instances_train2017_labeled.json',\n",
    "#                             transform=get_transform(train=True))\n",
    "\n",
    "val_dataset = COCODataset('/kaggle/input/coco-2017-dataset/coco2017/train2017',\n",
    "                          '/kaggle/input/semis-od-coco-10/instances_val2017.json',\n",
    "                          transform=get_transform(train=False))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f998089a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T22:50:02.763968Z",
     "iopub.status.busy": "2024-06-08T22:50:02.763684Z",
     "iopub.status.idle": "2024-06-08T22:50:09.855789Z",
     "shell.execute_reply": "2024-06-08T22:50:09.854857Z"
    },
    "papermill": {
     "duration": 7.100336,
     "end_time": "2024-06-08T22:50:09.858155",
     "exception": false,
     "start_time": "2024-06-08T22:50:02.757819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth\" to /root/.cache/torch/hub/checkpoints/ssd300_vgg16_coco-b556d3b4.pth\n",
      "100%|██████████| 136M/136M [00:03<00:00, 43.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import ssd300_vgg16\n",
    "\n",
    "model = ssd300_vgg16(pretrained=True)\n",
    "model.head.classification_head.num_classes = 9  # classes + background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b055ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T22:50:09.873796Z",
     "iopub.status.busy": "2024-06-08T22:50:09.873484Z",
     "iopub.status.idle": "2024-06-09T00:37:54.903943Z",
     "shell.execute_reply": "2024-06-09T00:37:54.902650Z"
    },
    "papermill": {
     "duration": 6465.048446,
     "end_time": "2024-06-09T00:37:54.913900",
     "exception": false,
     "start_time": "2024-06-08T22:50:09.865454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 loss: 4.013419828536061\n",
      "Epoch #2 loss: 3.300997937491181\n",
      "Epoch #3 loss: 3.028830558965975\n",
      "Epoch #4 loss: 2.8565238184212123\n",
      "Epoch #5 loss: 2.6956853338253692\n",
      "Epoch #6 loss: 2.583518311349792\n",
      "Epoch #7 loss: 2.4639912302761577\n",
      "Epoch #8 loss: 2.403932278730033\n",
      "Epoch #9 loss: 2.31091904059715\n",
      "Epoch #10 loss: 2.2300804149288482\n",
      "Epoch #11 loss: 2.1481233589566755\n",
      "Epoch #12 loss: 2.0997883832698654\n",
      "Epoch #13 loss: 2.0399920302709393\n",
      "Epoch #14 loss: 2.003468743073915\n",
      "Epoch #15 loss: 1.9364942769055309\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for images, targets, image_ids in train_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "        #print(loss_dict.values())\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_loss += losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch #{epoch + 1} loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'ssd_vgg16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfaf0fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:37:54.931537Z",
     "iopub.status.busy": "2024-06-09T00:37:54.931165Z",
     "iopub.status.idle": "2024-06-09T00:37:54.937071Z",
     "shell.execute_reply": "2024-06-09T00:37:54.936396Z"
    },
    "papermill": {
     "duration": 0.016921,
     "end_time": "2024-06-09T00:37:54.938807",
     "exception": false,
     "start_time": "2024-06-09T00:37:54.921886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb45eda8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:37:54.956032Z",
     "iopub.status.busy": "2024-06-09T00:37:54.955768Z",
     "iopub.status.idle": "2024-06-09T00:38:51.131147Z",
     "shell.execute_reply": "2024-06-09T00:38:51.130114Z"
    },
    "papermill": {
     "duration": 56.186509,
     "end_time": "2024-06-09T00:38:51.133296",
     "exception": false,
     "start_time": "2024-06-09T00:37:54.946787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.67s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=17.66s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=2.89s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.267\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.466\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.276\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.074\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.206\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.436\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.254\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.363\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.385\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.123\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.353\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.581\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "            for img_id, output in zip(image_ids, outputs):\n",
    "                boxes = output['boxes'].cpu().numpy()\n",
    "                scores = output['scores'].cpu().numpy()\n",
    "                labels = output['labels'].cpu().numpy()\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    result = {\n",
    "                        'image_id': img_id,\n",
    "                        'category_id': label,\n",
    "                        'bbox': [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                        'score': score\n",
    "                    }\n",
    "                    results.append(result)\n",
    "    return results\n",
    "\n",
    "results = evaluate_model(model, val_loader, device)\n",
    "\n",
    "# Load ground truth annotations\n",
    "coco_gt = COCO('/kaggle/input/semis-od-coco-10/instances_val2017.json')\n",
    "coco_dt = coco_gt.loadRes(results)\n",
    "\n",
    "# Initialize COCOeval\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42865140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T00:38:51.151891Z",
     "iopub.status.busy": "2024-06-09T00:38:51.151580Z",
     "iopub.status.idle": "2024-06-09T00:39:04.102225Z",
     "shell.execute_reply": "2024-06-09T00:39:04.100779Z"
    },
    "papermill": {
     "duration": 12.961968,
     "end_time": "2024-06-09T00:39:04.103935",
     "exception": true,
     "start_time": "2024-06-09T00:38:51.141967",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16_features-amdegroot-88682ab5.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_features-amdegroot-88682ab5.pth\n",
      "100%|██████████| 528M/528M [00:08<00:00, 69.0MB/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/testimage/boat.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/testimage/boat.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Display the image with predictions\u001b[39;00m\n\u001b[1;32m     65\u001b[0m display_predictions(image_path, predictions, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mpredict_image\u001b[0;34m(image_path, model, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_image\u001b[39m(image_path, model, device):\n\u001b[0;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     33\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(image)\n",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(image_path):\n\u001b[0;32m---> 23\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m     transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     25\u001b[0m         T\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m     26\u001b[0m     ])\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3236\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3237\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/testimage/boat.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.models.detection import ssd300_vgg16\n",
    "\n",
    "# Load the trained model\n",
    "model = ssd300_vgg16(pretrained=False)\n",
    "model.head.classification_head.num_classes = 9  # classes + background\n",
    "model.load_state_dict(torch.load('/kaggle/input/weightsod/ssd_vgg16_coco.pth'))\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat'\n",
    "]\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# Function to make predictions on a single image\n",
    "def predict_image(image_path, model, device):\n",
    "    image = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "    return outputs\n",
    "\n",
    "# Function to display the image with bounding boxes and labels\n",
    "def display_predictions(image_path, predictions, threshold=0.5):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    labels -= 1\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        if score >= threshold:  # Apply confidence threshold for visualization\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            width, height = x_max - x_min, y_max - y_min\n",
    "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x_min, y_min, f'{COCO_INSTANCE_CATEGORY_NAMES[label]}: {score:.2f}', bbox=dict(facecolor='yellow', alpha=0.5), fontsize=12, color='black')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Path to the image\n",
    "image_path = \"/kaggle/input/testimage/boat.jpg\"\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_image(image_path, model, device)\n",
    "\n",
    "# Display the image with predictions\n",
    "display_predictions(image_path, predictions, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff797f9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5110652,
     "sourceId": 8552185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5098546,
     "sourceId": 8578788,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4857582,
     "sourceId": 8599216,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6571.07268,
   "end_time": "2024-06-09T00:39:07.299602",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-08T22:49:36.226922",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
